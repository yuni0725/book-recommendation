{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from supabase import create_client\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "SUPABASE_URL = os.environ.get(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.environ.get(\"SUPABASE_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(query):\n",
    "    query_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\").embed_query(query)\n",
    "\n",
    "    response = supabase.rpc(\"match_book\", {\n",
    "        \"query_embedding\" : query_embedding, \"filter\": json.dumps({})\n",
    "        }).execute().data\n",
    "\n",
    "    context_texts = \"\\n\\n\".join([\n",
    "        f\"[{doc.get('name', 'ì œëª© ì—†ìŒ')}]\\n{doc['description']}\" for doc in response\n",
    "    ])\n",
    "\n",
    "    return context_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"\n",
    "         You are a professional book curator who recommends books by analyzing their summaries.\n",
    "        You understand the reader's intent from their search query, and you provide thoughtful recommendations with clear reasoning.\n",
    "\n",
    "        Below is the user's search query and a list of books whose summaries were retrieved based on that query.\n",
    "\n",
    "        ğŸ“Œ User's query:\n",
    "        \"{query}\"\n",
    "\n",
    "        ğŸ“š Relevant book summaries:\n",
    "        {context}\n",
    "\n",
    "        Please recommend books from the context that best match the user's query.\n",
    "        For each book, explain *why* it fits the query.\n",
    "\n",
    "        Use the tone, format, and detail level of the following example recommendation as a model:\n",
    "\n",
    "        âœ¨ Sample recommendation to imitate:\n",
    "        {book_recommendation}\n",
    "\n",
    "        Please write in Korean.\n",
    "        Now, generate your recommendation below in the same style:\n",
    "        \"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "query = \"ì² í•™\"\n",
    "chain = RunnablePassthrough() | question_prompt | llm\n",
    "\n",
    "result = chain.invoke({\"context\" : get_book(query), \"query\" : query, \"book_recommendation\" : \"None\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬ \"êµ¬ì˜ ì¦ëª…\"ê³¼ ê´€ë ¨í•˜ì—¬ ì í•©í•œ ì±…ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\\n\\n1. **ã€Šì˜¤ë¡œë¼ã€‹ - ìµœì§„ì˜**\\n   ì´ ì±…ì€ ìµœì§„ì˜ ì‘ê°€ì˜ ì‹ ì‘ ì†Œì„¤ë¡œ, ì œì£¼ë¥¼ ë°°ê²½ìœ¼ë¡œ í•œ ì‚¬ë‘ ì´ì•¼ê¸°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. \"êµ¬ì˜ ì¦ëª…\"ì´ë¼ëŠ” ì œëª©ì´ ì£¼ëŠ” ìˆ˜í•™ì  ë˜ëŠ” ì² í•™ì  ì˜ë¯¸ì™€ëŠ” ë‹¤ì†Œ ê±°ë¦¬ê°€ ìˆì§€ë§Œ, ì‚¬ë‘ì˜ ë³µì¡ì„±ê³¼ ì¸ê°„ì˜ ë‚´ë©´ì„ íƒêµ¬í•˜ëŠ” ë‚´ìš©ì´ ë…ìì˜ í˜¸ê¸°ì‹¬ì„ ìê·¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì£¼ì¸ê³µì´ ììœ ë¥¼ ì°¾ê³  ìƒˆë¡œìš´ ì •ì²´ì„±ì„ í˜•ì„±í•˜ëŠ” ê³¼ì •ì€ \"êµ¬ì˜ ì¦ëª…\"ì´ ê°–ëŠ” ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì‚¬ê³ ì™€ ì—°ê²°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì œì£¼ë¼ëŠ” ì¥ì†Œê°€ ì£¼ëŠ” ë…íŠ¹í•œ ë¶„ìœ„ê¸°ì™€ ê°ì •ì˜ ë³€í™”ëŠ” ë…ìê°€ ìƒˆë¡œìš´ ì‹œê°ì—ì„œ ì‚¬ë‘ì„ ë°”ë¼ë³´ê²Œ í•  ê²ƒì…ë‹ˆë‹¤.\\n\\nì´ ì™¸ì˜ ì±…ë“¤ì€ ì£¼ì œë‚˜ ë‚´ìš©ì´ \"êµ¬ì˜ ì¦ëª…\"ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ê±°ë‚˜, ìˆ˜í•™ì  ì‚¬ê³ ì™€ëŠ” ê±°ë¦¬ê°€ ìˆì–´ ì¶”ì²œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. \"ì˜¤ë¡œë¼\"ëŠ” ë…ìì˜ ê°ì •ê³¼ ì‚¬ê³ ë¥¼ ìê·¹í•  ìˆ˜ ìˆëŠ” ìš”ì†Œë¥¼ ì§€ë‹ˆê³  ìˆì–´, í¥ë¯¸ë¡œìš´ ë…ì„œ ê²½í—˜ì„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
